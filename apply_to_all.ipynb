{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.cluster import KMeans\n",
    "from matplotlib import pyplot as plt\n",
    "from math import sqrt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, random, sys\n",
    "\n",
    "# join result path\n",
    "JoinResPath = \"./data/JoinResults/\"\n",
    "# list of join result file names\n",
    "Ilist = [i for i in os.listdir(JoinResPath) if not i.startswith(\".\")]\n",
    "\n",
    "# read join result, if id is not given, randomly read one, \n",
    "# least only works when id is not given, i.e., if returned join result\n",
    "# does not contain enough points, it will recusively call itself again\n",
    "def readJR(Id):\n",
    "    Iid = (str(Id) if type(Id)==int else Id) if Id else random.choice(Ilist)\n",
    "    with open(JoinResPath+Iid,\"rb\") as f:\n",
    "        res = []\n",
    "        for i in f:\n",
    "            res.append(i.strip().split(\",\"))\n",
    "    df = pd.DataFrame(res[1:])\n",
    "    df[1],df[2],df[3] = pd.to_datetime(df[1]), df[2].astype(float), df[3].astype(float) \n",
    "    return (int(res[0][0]),float(res[0][1]),float(res[0][2])),df.sort([0,1])# sort_value doesn't work on server's version\n",
    "\n",
    "# plot scatter of join result\n",
    "# c is center of intersection, which will be ploted in red dot\n",
    "# df is the trajectory points in the form of pandas dataframe\n",
    "# kargs is the key parameters, that will be passed to the plt.scatter func\n",
    "# inorder to customize the scatter plot params in global scope\n",
    "def plotJR(c,df,**kargs):\n",
    "    plt.scatter(df[2],df[3],**kargs)\n",
    "    plt.scatter(float(c[1]),float(c[2]),color=\"r\",s=30)\n",
    "    xl,xu,yl,yu=df[2].min(),df[2].max(),df[3].min(),df[3].max()\n",
    "    xrng,yrng = xu-xl or 0.001,yu-yl or 0.001\n",
    "    plt.xlim([xl-.1*xrng,xu+.1*xrng])\n",
    "    plt.ylim([yl-.1*yrng,yu+.1*yrng])\n",
    "    plt.title(\"Intersection:%d, # of points:%d\"%(c[0],len(df)))\n",
    "\n",
    "# given a trajectories data frame, split it according to the time interval.\n",
    "# the rng is the params passed to the dateOffset, which will define the interval.\n",
    "# if two adjacent trajectory points' time intervel is grearter than this DateOffset,\n",
    "# it will be divided into to two trajectories\n",
    "# return result will be a list of dataframe, ordered by their first point's timestamp\n",
    "def spliTraj(df,**rng):\n",
    "    curr, last = None, None\n",
    "    currID, lastID = None, None\n",
    "    split=[0]\n",
    "    for i in xrange(df.shape[0]):\n",
    "        if type(curr)!=type(None):\n",
    "            last, lastID = curr,currID\n",
    "        curr,currID = df.iloc[i,1],df.iloc[i,0]\n",
    "        if type(last)!=type(None):\n",
    "            if curr>pd.DateOffset(**rng)+last or currID!=lastID:\n",
    "                split.append(i)\n",
    "    split.append(i+1)\n",
    "    #return split\n",
    "    ret = []\n",
    "    for i in xrange(1,len(split)):\n",
    "        splited=df.iloc[split[i-1]:split[i],:]\n",
    "        ret.append(splited)\n",
    "    return sorted(ret,cmp=lambda x,y:cmp(x.iloc[0,1],y.iloc[0,1]))\n",
    "    \n",
    "# Just like plotJR, but it will plot in the form of line.\n",
    "# this func takes the results from spliTraj, so dfs is the list of dataframe\n",
    "# c is still the center of the intersection, and kargs is for customizing the plot\n",
    "# params in global scope\n",
    "def plotJR_line(c,dfs,**kargs):\n",
    "    plt.scatter(c[1],c[2],color=\"r\",s=30)\n",
    "    if type(dfs)!=list:dfs=[dfs]\n",
    "    for df in dfs: \n",
    "        for t in set(df.iloc[:,0]):    \n",
    "            if len(df)>1:plt.plot(df.iloc[:,2],df.iloc[:,3],**kargs)\n",
    "    dfs=pd.concat(dfs)\n",
    "    xl,xu,yl,yu=dfs[2].min(),dfs[2].max(),dfs[3].min(),dfs[3].max() \n",
    "    xrng,yrng = xu-xl or 0.001,yu-yl or 0.001\n",
    "    plt.xlim([xl-.1*xrng,xu+.1*xrng])\n",
    "    plt.ylim([yl-.1*yrng,yu+.1*yrng])\n",
    "    plt.title(\"Intersection:%d, # of points:%d\"%(c[0],len(dfs)))\n",
    "\n",
    "# given a range and start time, filter the df (range qurey).\n",
    "# no rng return df itself, \n",
    "# df could be a list of df, that actually return from splitTraj,\n",
    "# or it is just a df, not list. No matter what, func will just\n",
    "# filter the row of the dataframe by time range query mask\n",
    "def timeRange(df,time,**rng):\n",
    "    if len(rng)==0:return df\n",
    "    if type(df)!=list: \n",
    "        mask = df.ix[:,1]<time+pd.DateOffset(**rng)\n",
    "        mask &= df.ix[:,1]>=time\n",
    "        return None if df.ix[mask,:].empty else df.ix[mask,:]\n",
    "    else:\n",
    "        ret = []\n",
    "        for i in df:\n",
    "            tmp=timeRange(i,time,**rng)\n",
    "            if type(tmp)!=type(None):ret.append(tmp)\n",
    "        return ret\n",
    "\n",
    "# given the result from splitTraj, extract the info of intervals, like speed,\n",
    "# directions, the direction will be normalized into unit vector\n",
    "# the interval info will be called features, to contribute the inferring \n",
    "# verbose tells the progress, but makes cell messy\n",
    "def calc_features(dfs,verbose=0,c=None):\n",
    "    features=[]\n",
    "    count=0\n",
    "    for df in dfs:\n",
    "        if len(df)<2:continue\n",
    "        # interval infos.\n",
    "        tdiff,xdiff,ydiff = [df[i].diff().iloc[1:] for i in (1,2,3)]\n",
    "        # distances\n",
    "        d = list(np.sqrt(xdiff**2+ydiff**2))\n",
    "        # direction vectors\n",
    "        xd = [0 if np.isnan(i) else i for i in (np.divide(xdiff,d))]\n",
    "        yd = [0 if np.isnan(i) else i for i in (np.divide(ydiff,d))]\n",
    "        # speed\n",
    "        v = list(d/tdiff.astype('timedelta64[s]'))\n",
    "        stacked = [v,xd,yd]\n",
    "        # distances from intersections\n",
    "        if c:\n",
    "            x_from_center, y_from_center = np.array(df[2]-c[1]), np.array(df[3]-c[2])\n",
    "            dist_sq_from_center = x_from_center**2+y_from_center**2\n",
    "            stacked.append(dist_sq_from_center[1:]+dist_sq_from_center[:-1])\n",
    "        features.append({\"time\":list(df[1]),\"status\":np.vstack(stacked).T})\n",
    "        if verbose:\n",
    "            count+=1\n",
    "            print \"\\r%d/%d\" % (count,len(dfs))\n",
    "    return features\n",
    "\n",
    "# tic tac ...\n",
    "import time\n",
    "class Timer():\n",
    "    def __init__(self,msg_last=\"elasped time:\",verbose=1):\n",
    "        self.msg=\"\"\n",
    "        self.msg_last=msg_last\n",
    "        self.verbose=verbose\n",
    "    def __enter__(self):\n",
    "        self.start = time.time()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        if self.verbose:\n",
    "            self.end = time.time()\n",
    "            self.interval = self.end - self.start\n",
    "            print self.msg+self.msg_last,\"%.1f\"%self.interval\n",
    "            sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "# def some func to save results\n",
    "def dump(obj,path):\n",
    "    with open(path,\"wb\") as f:\n",
    "        pickle.dump(obj,f,protocol=pickle.HIGHEST_PROTOCOL)\n",
    "def load(path):\n",
    "    with open(path,\"rb\") as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SPLIT_INTERVAL = {\"minutes\":0,\"hours\":0,\"seconds\":30}\n",
    "TIME_RNG = {\"hours\":24}\n",
    "#??????????????\n",
    "DIRECTION_NUM=4# ? how to select it automatically\n",
    "#??????????????\n",
    "COS_POW = 2 # set 0 to turn off direction, higher more weighted by one direction\n",
    "DIST_VAR = 1e-3 # set 1 to make it nearly no difference, less means more weighted by distance\n",
    "transform_factor = 10\n",
    "\n",
    "def execute(_id,verbose=0):\n",
    "    def monitoring(monitor,feature,direction): \n",
    "        starts = [(i-pd.Timestamp(\"00:00:00\")).seconds for i in feature[\"time\"]]\n",
    "        y = feature[\"status\"][:,0]/np.exp(feature[\"status\"][:,3]/DIST_VAR)\n",
    "        if type(direction) != type(None):\n",
    "            cosine = np.dot(feature[\"status\"][:,1:3],direction)/sqrt(2)\n",
    "            similarity = map(lambda x:max([0,x]),cosine**COS_POW)\n",
    "            y = y*similarity\n",
    "        for i in xrange(len(y)):\n",
    "            for j in xrange(starts[i],starts[i+1]):\n",
    "                monitor[j].append(y[i])\n",
    "\n",
    "    def wrapped(x):\n",
    "        if len(x)==0:return 0\n",
    "        return np.nanmean([i for i in x if np.isfinite(i)])\n",
    "    # reading\n",
    "    with Timer(\"reading:\",verbose):\n",
    "        center,points = readJR(_id)\n",
    "    # print \"number of points:\",len(points),\"...\",\n",
    "    if len(points)<1000:return None\n",
    "    # spliting\n",
    "    with Timer(\"spliting:\",verbose):\n",
    "        splitTJ = spliTraj(points,**SPLIT_INTERVAL)\n",
    "    # filtering\n",
    "    with Timer(\"filtering:\",verbose):\n",
    "        rngres = timeRange(splitTJ,pd.Timestamp(\"00:00:00\"),**TIME_RNG)\n",
    "    # calculating interval attrs\n",
    "    with Timer(\"features:\",verbose):\n",
    "        features=calc_features(rngres,c=center)\n",
    "    # directions\n",
    "    with Timer(\"dir:\",verbose):\n",
    "        cuml_dirs = []\n",
    "        for i in features:\n",
    "            filter_ = i[\"status\"][:,1:3].sum(axis=1)!=0.0\n",
    "            ##remove 1,0\n",
    "            filter_ &= abs(i[\"status\"][:,1:3].sum(axis=1))!=1\n",
    "            cuml_dirs.append(i[\"status\"][filter_,1:3])\n",
    "        cuml_dirs = np.vstack(cuml_dirs)\n",
    "        model = KMeans(n_clusters=DIRECTION_NUM)\n",
    "        model.fit(cuml_dirs)\n",
    "        directions = model.cluster_centers_   \n",
    "        labels = model.labels_\n",
    "    # monitoring\n",
    "    with Timer(\"monitoring:\",verbose):\n",
    "        speed_monitors=[]            \n",
    "        for direction in directions:\n",
    "            speed_monitor=[[] for i in xrange(86400)]\n",
    "            for feature in features:\n",
    "                monitoring(speed_monitor,feature,direction)\n",
    "            speed_monitors.append(speed_monitor)\n",
    "        # trends   \n",
    "        trends=[]\n",
    "        for i in xrange(len(directions)):\n",
    "            trend = []\n",
    "            for j in speed_monitors[i]:\n",
    "                trend.append(wrapped(j))\n",
    "            trends.append(1-1/(1+transform_factor*len(points)*np.array(trend)))\n",
    "#---collecting result---#\n",
    "    collector={}\n",
    "    collector[\"len\"]=len(points)\n",
    "    collector[\"dirs\"]=directions\n",
    "    collector[\"features\"]=features\n",
    "    collector[\"trends\"]=trends\n",
    "    collector[\"coef\"]=np.corrcoef(trends)\n",
    "#-----------------------# \n",
    "    return collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 left\n",
      "Start multiprocessing model\n",
      "Job assigned\n",
      "All set!\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Process\n",
    "Monitoring_PATH = \"./data/MoniteringResults/\" \n",
    "\n",
    "# set true to run multi processing\n",
    "MULTIPLE=True\n",
    "# number of processors\n",
    "nthread=24\n",
    "# get unfinished jobs\n",
    "unfinished = list(set(Ilist)-set(os.listdir(Monitoring_PATH)))\n",
    "print len(unfinished),\"left\"\n",
    "\n",
    "# give each processor the job\n",
    "# job is executor + part of unfinished list\n",
    "def job_generator(executor,joblist,thread_id):\n",
    "    def func():\n",
    "        for _id in joblist:\n",
    "            executor(_id,thread_id)\n",
    "    return func\n",
    "\n",
    "# given _id(intersection id), run the work plus some prints\n",
    "def executor(_id,thread_id=0):\n",
    "    with Timer() as t:\n",
    "        t.msg+=\"Executed by thread %d #%s ...\"%(thread_id,_id)\n",
    "        if _id in os.listdir(Monitoring_PATH): \n",
    "            t.msg+= \" already finished. \"\n",
    "            return\n",
    "        res = execute(_id)\n",
    "        if not res: \n",
    "            t.msg+=\" Too small, jumped. \"\n",
    "            dump(res,Monitoring_PATH+str(_id))\n",
    "            return\n",
    "        t.msg+= (\"# of points: %d \"%res[\"len\"])\n",
    "        dump(res,Monitoring_PATH+str(_id))\n",
    "        t.msg+=\"done. \"\n",
    "\n",
    "# auto-start/auto-terminate class for processors\n",
    "class p_exit:\n",
    "    def __init__(self,quene):\n",
    "        self.quene=quene\n",
    "    def __enter__(self):\n",
    "        for q in quene:\n",
    "            q.start()\n",
    "    def __exit__(self,*arg):\n",
    "        for q in quene:\n",
    "            q.terminate()\n",
    "        \n",
    "if MULTIPLE:\n",
    "    print \"Start multiprocessing model\"\n",
    "    avg_length = len(unfinished)/nthread\n",
    "    quene=[]\n",
    "    for i in xrange(nthread):\n",
    "        if i+1==nthread:# last handle rest\n",
    "            func = job_generator(executor,unfinished[i*avg_length:],i)\n",
    "        else:\n",
    "            func = job_generator(executor,unfinished[i*avg_length:(i+1)*avg_length],i)\n",
    "        p=Process(target=func)\n",
    "        quene.append(p)\n",
    "    print \"Job assigned\"\n",
    "    with p_exit(quene):\n",
    "        for q in quene:\n",
    "            q.join()\n",
    "else:             \n",
    "    for _id in sorted(Ilist):\n",
    "        executor(_id)\n",
    "    \n",
    "print \"All set!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_id = 1117\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plotJR(*readJR(_id),s=1,color=\"b\",alpha=.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1157 1158 1159 116 1160 1162 1163 1165 1166 1167 1168 1169 1173 1174 1175 1177 1178 1179 118 1180 1181 1182 1185 1186 1187 1188 1189 119 1190 1192 1193 1194 1195 1196 1197 1198 1199 120 1200 1201 1202 1203 1205 1206 121 1211 1212 1213 1214 1215 1216 1217 1219 122 1220 1221 1222 1223 1227 123 1230 1231 1232 1233 1234 1237 1238 1240 1242 1243 1244 1245 1246 1248 125 1250 1251 1252 1254 1255 1256 1258 1260 1261 1262 1263 1265 1267 1268 1269 127 1271 1272 1274 1275 1276 1277 1278 1279 128 1281 1282 1283 1284 1286 1287 1288 1289 129 1290 1291 1292 1293 1294 1296 1297 1298 1299 13 130 1300 1301 1302 1303 1307 1308 1309 131 1310 1311 1312 1313 1314 1316 1317 1319 132 1320 1321 1322 1324 1325 1326 1327 1328 1329 133 1330 1332 1334 1335 1336 1337 1338 1339 134 1341 1342 1343 1344 1345 1346 1347 1348 135 1350 1352 1353 1355 1356 1357 1359 1360 1361 1362 1363 1364 1365 1366 1367 1369 137 1371 1373 1375 1377 1378 1379 138 1380 1382 1384 1385 1386 1388 139 1390 1391 1393 1394 1395 1396 1397 1399 140 1400 1401 1402 1404 1405 1406 1407 1409 1410 1411 1412 1416 1418 1419 142 1420 1421 1422 1423 1424 1426 1427 1428 1430 1431 1432 1434 1435 1436 1438 1439 144 1440 1441 1442 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1456 1457 1458 1459 1460\n"
     ]
    }
   ],
   "source": [
    "start,end = 170,500\n",
    "Monitoring_PATH = \"./data/MoniteringResults/\" \n",
    "\n",
    "for _id in [] or sorted(os.listdir(Monitoring_PATH))[start:end]:\n",
    "    res=load(Monitoring_PATH+str(_id))\n",
    "    if res is None:continue\n",
    "    else: \n",
    "        print _id,\n",
    "        res[\"len\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "!rm data/MoniteringResults/1172\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
