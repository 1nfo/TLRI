{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.cluster import KMeans\n",
    "from matplotlib import pyplot as plt\n",
    "from math import sqrt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, random, sys\n",
    "\n",
    "# join result path\n",
    "JoinResPath = \"./data/JoinResults/\"\n",
    "# list of join result file names\n",
    "Ilist = [i for i in os.listdir(JoinResPath) if not i.startswith(\".\")]\n",
    "\n",
    "# read join result, if id is not given, randomly read one, \n",
    "# least only works when id is not given, i.e., if returned join result\n",
    "# does not contain enough points, it will recusively call itself again\n",
    "def readJR(Id):\n",
    "    Iid = (str(Id) if type(Id)==int else Id) if Id else random.choice(Ilist)\n",
    "    with open(JoinResPath+Iid,\"rb\") as f:\n",
    "        res = []\n",
    "        for i in f:\n",
    "            res.append(i.strip().split(\",\"))\n",
    "    df = pd.DataFrame(res[1:])\n",
    "    df[1],df[2],df[3] = pd.to_datetime(df[1]), df[2].astype(float), df[3].astype(float) \n",
    "    return (int(res[0][0]),float(res[0][1]),float(res[0][2])),df.sort([0,1])# sort_value doesn't work on server's version\n",
    "\n",
    "# plot scatter of join result\n",
    "# c is center of intersection, which will be ploted in red dot\n",
    "# df is the trajectory points in the form of pandas dataframe\n",
    "# kargs is the key parameters, that will be passed to the plt.scatter func\n",
    "# inorder to customize the scatter plot params in global scope\n",
    "def plotJR(c,df,**kargs):\n",
    "    plt.scatter(df[2],df[3],**kargs)\n",
    "    plt.scatter(float(c[1]),float(c[2]),color=\"r\",s=30)\n",
    "    xl,xu,yl,yu=df[2].min(),df[2].max(),df[3].min(),df[3].max()\n",
    "    xrng,yrng = xu-xl or 0.001,yu-yl or 0.001\n",
    "    plt.xlim([xl-.1*xrng,xu+.1*xrng])\n",
    "    plt.ylim([yl-.1*yrng,yu+.1*yrng])\n",
    "    plt.title(\"Intersection:%d, # of points:%d\"%(c[0],len(df)))\n",
    "\n",
    "# given a trajectories data frame, split it according to the time interval.\n",
    "# the rng is the params passed to the dateOffset, which will define the interval.\n",
    "# if two adjacent trajectory points' time intervel is grearter than this DateOffset,\n",
    "# it will be divided into to two trajectories\n",
    "# return result will be a list of dataframe, ordered by their first point's timestamp\n",
    "def spliTraj(df,**rng):\n",
    "    curr, last = None, None\n",
    "    currID, lastID = None, None\n",
    "    split=[0]\n",
    "    for i in xrange(df.shape[0]):\n",
    "        if type(curr)!=type(None):\n",
    "            last, lastID = curr,currID\n",
    "        curr,currID = df.iloc[i,1],df.iloc[i,0]\n",
    "        if type(last)!=type(None):\n",
    "            if curr>pd.DateOffset(**rng)+last or currID!=lastID:\n",
    "                split.append(i)\n",
    "    split.append(i+1)\n",
    "    #return split\n",
    "    ret = []\n",
    "    for i in xrange(1,len(split)):\n",
    "        splited=df.iloc[split[i-1]:split[i],:]\n",
    "        ret.append(splited)\n",
    "    return sorted(ret,cmp=lambda x,y:cmp(x.iloc[0,1],y.iloc[0,1]))\n",
    "    \n",
    "# Just like plotJR, but it will plot in the form of line.\n",
    "# this func takes the results from spliTraj, so dfs is the list of dataframe\n",
    "# c is still the center of the intersection, and kargs is for customizing the plot\n",
    "# params in global scope\n",
    "def plotJR_line(c,dfs,**kargs):\n",
    "    plt.scatter(c[1],c[2],color=\"r\",s=30)\n",
    "    if type(dfs)!=list:dfs=[dfs]\n",
    "    for df in dfs: \n",
    "        for t in set(df.iloc[:,0]):    \n",
    "            if len(df)>1:plt.plot(df.iloc[:,2],df.iloc[:,3],**kargs)\n",
    "    dfs=pd.concat(dfs)\n",
    "    xl,xu,yl,yu=dfs[2].min(),dfs[2].max(),dfs[3].min(),dfs[3].max() \n",
    "    xrng,yrng = xu-xl or 0.001,yu-yl or 0.001\n",
    "    plt.xlim([xl-.1*xrng,xu+.1*xrng])\n",
    "    plt.ylim([yl-.1*yrng,yu+.1*yrng])\n",
    "    plt.title(\"Intersection:%d, # of points:%d\"%(c[0],len(dfs)))\n",
    "\n",
    "# given a range and start time, filter the df (range qurey).\n",
    "# no rng return df itself, \n",
    "# df could be a list of df, that actually return from splitTraj,\n",
    "# or it is just a df, not list. No matter what, func will just\n",
    "# filter the row of the dataframe by time range query mask\n",
    "def timeRange(df,time,**rng):\n",
    "    if len(rng)==0:return df\n",
    "    if type(df)!=list: \n",
    "        mask = df.ix[:,1]<time+pd.DateOffset(**rng)\n",
    "        mask &= df.ix[:,1]>=time\n",
    "        return None if df.ix[mask,:].empty else df.ix[mask,:]\n",
    "    else:\n",
    "        ret = []\n",
    "        for i in df:\n",
    "            tmp=timeRange(i,time,**rng)\n",
    "            if type(tmp)!=type(None):ret.append(tmp)\n",
    "        return ret\n",
    "\n",
    "# given the result from splitTraj, extract the info of intervals, like speed,\n",
    "# directions, the direction will be normalized into unit vector\n",
    "# the interval info will be called features, to contribute the inferring \n",
    "# verbose tells the progress, but makes cell messy\n",
    "def calc_features(dfs,verbose=0,c=None):\n",
    "    features=[]\n",
    "    count=0\n",
    "    for df in dfs:\n",
    "        if len(df)<2:continue\n",
    "        # interval infos.\n",
    "        tdiff,xdiff,ydiff = [df[i].diff().iloc[1:] for i in (1,2,3)]\n",
    "        # distances\n",
    "        d = list(np.sqrt(xdiff**2+ydiff**2))\n",
    "        # direction vectors\n",
    "        xd = [0 if np.isnan(i) else i for i in (np.divide(xdiff,d))]\n",
    "        yd = [0 if np.isnan(i) else i for i in (np.divide(ydiff,d))]\n",
    "        # speed\n",
    "        v = list(d/tdiff.astype('timedelta64[s]'))\n",
    "        stacked = [v,xd,yd]\n",
    "        # distances from intersections\n",
    "        if c:\n",
    "            x_from_center, y_from_center = np.array(df[2]-c[1]), np.array(df[3]-c[2])\n",
    "            dist_sq_from_center = x_from_center**2+y_from_center**2\n",
    "            stacked.append(dist_sq_from_center[1:]+dist_sq_from_center[:-1])\n",
    "        features.append({\"time\":list(df[1]),\"status\":np.vstack(stacked).T})\n",
    "        if verbose:\n",
    "            count+=1\n",
    "            print \"\\r%d/%d\" % (count,len(dfs))\n",
    "    return features\n",
    "\n",
    "import time\n",
    "class Timer():\n",
    "    def __init__(self,msg_last=\"elasped time:\",verbose=1):\n",
    "        self.msg=\"\"\n",
    "        self.msg_last=msg_last\n",
    "        self.verbose=verbose\n",
    "    def __enter__(self):\n",
    "        self.start = time.time()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        if self.verbose:\n",
    "            self.end = time.time()\n",
    "            self.interval = self.end - self.start\n",
    "            print self.msg+self.msg_last,\"%.1f\"%self.interval\n",
    "            sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "# def some func to save results\n",
    "def dump(obj,path):\n",
    "    with open(path,\"wb\") as f:\n",
    "        pickle.dump(obj,f,protocol=pickle.HIGHEST_PROTOCOL)\n",
    "def load(path):\n",
    "    with open(path,\"rb\") as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SPLIT_INTERVAL = {\"minutes\":0,\"hours\":0,\"seconds\":30}\n",
    "TIME_RNG = {\"hours\":24}\n",
    "#??????????????\n",
    "DIRECTION_NUM=4# ? how to select it automatically\n",
    "#??????????????\n",
    "COS_POW = 2 # set 0 to turn off direction, higher more weighted by one direction\n",
    "DIST_VAR = 1e-3 # set 1 to make it nearly no difference, less means more weighted by distance\n",
    "transform_factor = 10\n",
    "\n",
    "def execute(_id,verbose=0):\n",
    "    def monitoring(monitor,feature,direction): \n",
    "        starts = [(i-pd.Timestamp(\"00:00:00\")).seconds for i in feature[\"time\"]]\n",
    "        y = feature[\"status\"][:,0]/np.exp(feature[\"status\"][:,3]/DIST_VAR)\n",
    "        if type(direction) != type(None):\n",
    "            cosine = np.dot(feature[\"status\"][:,1:3],direction)/sqrt(2)\n",
    "            similarity = map(lambda x:max([0,x]),cosine**COS_POW)\n",
    "            y = y*similarity\n",
    "        for i in xrange(len(y)):\n",
    "            for j in xrange(starts[i],starts[i+1]):\n",
    "                monitor[j].append(y[i])\n",
    "\n",
    "    def wrapped(x):\n",
    "        if len(x)==0:return 0\n",
    "        return np.nanmean([i for i in x if np.isfinite(i)])\n",
    "    # reading\n",
    "    with Timer(\"reading:\",verbose):\n",
    "        center,points = readJR(_id)\n",
    "    # print \"number of points:\",len(points),\"...\",\n",
    "    if len(points)<1000:return None\n",
    "    # spliting\n",
    "    with Timer(\"spliting:\",verbose):\n",
    "        splitTJ = spliTraj(points,**SPLIT_INTERVAL)\n",
    "    # filtering\n",
    "    with Timer(\"filtering:\",verbose):\n",
    "        rngres = timeRange(splitTJ,pd.Timestamp(\"00:00:00\"),**TIME_RNG)\n",
    "    # calculating interval attrs\n",
    "    with Timer(\"features:\",verbose):\n",
    "        features=calc_features(rngres,c=center)\n",
    "    # directions\n",
    "    with Timer(\"dir:\",verbose):\n",
    "        cuml_dirs = []\n",
    "        for i in features:\n",
    "            filter_ = i[\"status\"][:,1:3].sum(axis=1)!=0.0\n",
    "            ##remove 1,0\n",
    "            filter_ &= abs(i[\"status\"][:,1:3].sum(axis=1))!=1\n",
    "            cuml_dirs.append(i[\"status\"][filter_,1:3])\n",
    "        cuml_dirs = np.vstack(cuml_dirs)\n",
    "        model = KMeans(n_clusters=DIRECTION_NUM)\n",
    "        model.fit(cuml_dirs)\n",
    "        directions = model.cluster_centers_   \n",
    "        labels = model.labels_\n",
    "    # monitoring\n",
    "    with Timer(\"monitoring:\",verbose):\n",
    "        speed_monitors=[]            \n",
    "        for direction in directions:\n",
    "            speed_monitor=[[] for i in xrange(86400)]\n",
    "            for feature in features:\n",
    "                monitoring(speed_monitor,feature,direction)\n",
    "            speed_monitors.append(speed_monitor)\n",
    "        # trends   \n",
    "        trends=[]\n",
    "        for i in xrange(len(directions)):\n",
    "            trend = []\n",
    "            for j in speed_monitors[i]:\n",
    "                trend.append(wrapped(j))\n",
    "            trends.append(1-1/(1+transform_factor*len(points)*np.array(trend)))\n",
    "#---collecting result---#\n",
    "    collector={}\n",
    "    collector[\"len\"]=len(points)\n",
    "    collector[\"dirs\"]=directions\n",
    "    collector[\"features\"]=features\n",
    "    collector[\"trends\"]=trends\n",
    "    collector[\"coef\"]=np.corrcoef(trends)\n",
    "#-----------------------# \n",
    "    return collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start multiprocessing model\n",
      "Job assigned\n",
      "Executed by thread 0 #2031 ... Too small, jumped. elasped time: 0.0\n",
      "Executed by thread 2 #2457 ... Too small, jumped. elasped time: 0.0\n",
      "Executed by thread 6 #4195 ... Too small, jumped. elasped time: 0.0\n",
      "Executed by thread 0 #965 ... Too small, jumped. elasped time: 0.0\n",
      "Executed by thread 6 #4193 ... Too small, jumped. elasped time: 0.0\n",
      "Executed by thread 0 #4022 ... Too small, jumped. elasped time: 0.0\n",
      "Executed by thread 6 #4190 ... Too small, jumped. elasped time: 0.0\n",
      "Executed by thread 0 #4029 ... Too small, jumped. elasped time: 0.0\n",
      "Executed by thread 12 #3294 ... Too small, jumped. elasped time: 0.0\n",
      "Executed by thread 0 #1030 ... Too small, jumped. elasped time: 0.0\n",
      "Executed by thread 16 #3543 ... Too small, jumped. elasped time: 0.0\n",
      "Executed by thread 12 #5135 ... Too small, jumped. elasped time: 0.0\n",
      "Executed by thread 6 #3954 ... Too small, jumped. elasped time: 0.0\n",
      "Executed by thread 18 #4322 ... Too small, jumped. elasped time: 0.0\n",
      "Executed by thread 16 #3548 ... Too small, jumped. elasped time: 0.0\n",
      "Executed by thread 6 #4352 ... Too small, jumped. elasped time: 0.0\n",
      "Executed by thread 18 #4326 ... Too small, jumped. elasped time: 0.0\n",
      "Executed by thread 0 #345 ... Too small, jumped. elasped time: 0.0\n",
      "Executed by thread 0 #4931 ... Too small, jumped. elasped time: 0.0\n",
      "Executed by thread 0 #966 ... Too small, jumped. elasped time: 0.0\n",
      "Executed by thread 17 #3051 ... Too small, jumped. elasped time: 0.0\n",
      "Executed by thread 23 #2074 ... Too small, jumped. elasped time: 0.0\n",
      "Executed by thread 0 #967 ... Too small, jumped. elasped time: 0.0\n",
      "Executed by thread 6 #4353 ... Too small, jumped. elasped time: 0.0\n",
      "Executed by thread 6 #2488 ... Too small, jumped. elasped time: 0.0\n",
      "Executed by thread 0 #5158 ... Too small, jumped. elasped time: 0.0\n",
      "Executed by thread 18 #3144 ... Too small, jumped. elasped time: 0.1\n",
      "Executed by thread 6 #2489 ... Too small, jumped. elasped time: 0.1\n",
      "Executed by thread 18 #2825 ... Too small, jumped. elasped time: 0.0\n",
      "Executed by thread 6 #438 ... Too small, jumped. elasped time: 0.0\n",
      "Executed by thread 6 #439 ... Too small, jumped. elasped time: 0.0\n",
      "Executed by thread 6 #435 ... Too small, jumped. elasped time: 0.0\n",
      "Executed by thread 6 #433 ... Too small, jumped. elasped time: 0.0\n",
      "Executed by thread 6 #3809 ... Too small, jumped. elasped time: 0.0\n",
      "Executed by thread 6 #2481 ... Too small, jumped. elasped time: 0.0\n",
      "Executed by thread 16 #641 ...# of points: 1124 done. elasped time: 5.8\n",
      "Executed by thread 4 #2128 ...# of points: 1047 done. elasped time: 6.1\n",
      "Executed by thread 20 #1341 ...# of points: 5167 done. elasped time: 30.8\n",
      "Executed by thread 14 #884 ...# of points: 10907 done. elasped time: 32.2\n",
      "Executed by thread 20 #1342 ...# of points: 1251 done. elasped time: 7.2\n",
      "Executed by thread 13 #3847 ...# of points: 5788 done. elasped time: 41.7\n",
      "Executed by thread 13 #3846 ... Too small, jumped. elasped time: 0.0\n",
      "Executed by thread 13 #3849 ... Too small, jumped. elasped time: 0.0\n",
      "Executed by thread 13 #4431 ... Too small, jumped. elasped time: 0.0\n",
      "Executed by thread 13 #1873 ... Too small, jumped. elasped time: 0.0\n",
      "Executed by thread 13 #2136 ... Too small, jumped. elasped time: 0.0\n",
      "Executed by thread 21 #996 ...# of points: 8040 done. elasped time: 49.4\n",
      "Executed by thread 20 #1888 ...# of points: 1798 done. elasped time: 12.6\n",
      "Executed by thread 7 #3247 ...# of points: 8734 done. elasped time: 55.9\n",
      "Executed by thread 7 #828 ... Too small, jumped. elasped time: 0.0\n",
      "Executed by thread 7 #1824 ... Too small, jumped. elasped time: 0.1\n",
      "Executed by thread 7 #825 ... Too small, jumped. elasped time: 0.0\n",
      "Executed by thread 7 #827 ... Too small, jumped. elasped time: 0.1\n",
      "Executed by thread 17 #3602 ...# of points: 8619 done. elasped time: 61.3\n",
      "Executed by thread 6 #185 ...# of points: 8513 done. elasped time: 69.1\n",
      "Executed by thread 0 #512 ...# of points: 10803 done. elasped time: 71.9\n",
      "Executed by thread 22 #1519 ...# of points: 15674 done. elasped time: 94.2\n",
      "Executed by thread 22 #2013 ... Too small, jumped. elasped time: 0.0\n",
      "Executed by thread 23 #1388 ...# of points: 17742 done. elasped time: 97.1\n",
      "Executed by thread 3 #1380 ...# of points: 18610 done. elasped time: 98.9\n",
      "Executed by thread 5 #3698 ...# of points: 17315 done. elasped time: 103.2\n",
      "Executed by thread 5 #151 ...# of points: 1212 done. elasped time: 8.2\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Process\n",
    "Monitoring_PATH = \"./data/MoniteringResults/\" \n",
    "\n",
    "MULTIPLE=True\n",
    "nthread=24\n",
    "unfinished = list(set(Ilist)-set(os.listdir(Monitoring_PATH)))\n",
    "\n",
    "\n",
    "def job_generator(executor,joblist,thread_id):\n",
    "    def func():\n",
    "        for _id in joblist:\n",
    "            executor(_id,thread_id)\n",
    "    return func\n",
    "\n",
    "def executor(_id,thread_id=0):\n",
    "    with Timer() as t:\n",
    "        t.msg+=\"Executed by thread %d #%s ...\"%(thread_id,_id)\n",
    "        if _id in os.listdir(Monitoring_PATH): \n",
    "            t.msg+= \" already finished. \"\n",
    "            return\n",
    "        res = execute(_id)\n",
    "        if not res: \n",
    "            t.msg+=\" Too small, jumped. \"\n",
    "            return\n",
    "        t.msg+= (\"# of points: %d \"%res[\"len\"])\n",
    "        dump(res,Monitoring_PATH+str(_id))\n",
    "        t.msg+=\"done. \"\n",
    "\n",
    "class p_exit:\n",
    "    def __init__(self,quene):\n",
    "        self.quene=quene\n",
    "    def __enter__(self):\n",
    "        for q in quene:\n",
    "            q.start()\n",
    "    def __exit__(self,*arg):\n",
    "        for q in quene:\n",
    "            q.terminate()\n",
    "        \n",
    "if MULTIPLE:\n",
    "    print \"Start multiprocessing model\"\n",
    "    avg_length = len(unfinished)/nthread\n",
    "    quene=[]\n",
    "    for i in xrange(nthread):\n",
    "        if i+1==nthread:\n",
    "            func = job_generator(executor,unfinished[i*avg_length:],i)\n",
    "        else:\n",
    "            func = job_generator(executor,unfinished[i*avg_length:(i+1)*avg_length],i)\n",
    "        p=Process(target=func)\n",
    "        quene.append(p)\n",
    "    print \"Job assigned\"\n",
    "    with p_exit(quene):\n",
    "        for q in quene:\n",
    "            q.join()\n",
    "else:             \n",
    "    for _id in sorted(Ilist):\n",
    "        executor(_id)\n",
    "    \n",
    "print \"All set!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_id = 1117\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plotJR(*readJR(_id),s=1,color=\"b\",alpha=.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start,end = 0,None\n",
    "\n",
    "for _id in [] or sorted(os.listdir(Monitoring_PATH))[start:end]:\n",
    "    print _id\n",
    "    res=load(Monitoring_PATH+str(_id))\n",
    "    plt.matshow(1-res[\"coef\"],cmap=plt.cm.gray)\n",
    "    plt.show()\n",
    "    for trend in res[\"trends\"]:\n",
    "        plt.figure(figsize=(20,.5))\n",
    "        plt.plot(trend[40000:40300])\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.show()\n",
    "print \"All set!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
